{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Lecture 9 Exercise 1\n",
    "\n",
    "What is the total frequency search range for L2 and L5 carrier, if the TCXO has the following specifications:\n",
    "\n",
    "TCXO offset within +-3ppm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcxo_offset = 3e-6\n",
    "f_L1 = 1575.42e6\n",
    "f_L2 = 1227.60e6\n",
    "f_L5 = 1176.45e6\n",
    "f_L1_offset = f_L1 * tcxo_offset\n",
    "f_L2_offset = f_L2 * tcxo_offset\n",
    "f_L5_offset = f_L5 * tcxo_offset\n",
    "print(f\"L5 Doppler search range: {-f_L5_offset} Hz to {f_L5_offset} Hz\")\n",
    "print(f\"L2 Doppler search range: {-f_L2_offset} Hz to {f_L2_offset} Hz\")\n",
    "print(f\"L1 Doppler search range: {-f_L1_offset} Hz to {f_L1_offset} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Lecture 9 Exercise 2\n",
    "\n",
    "How long does it take for an L2C code and an L5 code to mis-align by ½ chip due to Doppler frequency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_rate_L2C = 1.023e6  # chips per second\n",
    "code_rate_L5 = 10.23e6  # chips per second\n",
    "\n",
    "f_L1 = 1575.42e6\n",
    "f_L2 = 1227.60e6\n",
    "f_L5 = 1176.45e6\n",
    "speed_of_light = 299792458  # meters per second\n",
    "v_LOS_eff = speed_of_light * tcxo_offset  # effective TCXO rate in m/s\n",
    "\n",
    "num_steps = 1000\n",
    "time_rate_offset = np.linspace(-tcxo_offset, tcxo_offset, num_steps)  # Hz\n",
    "doppler_arr_L2C = f_L2 * time_rate_offset\n",
    "doppler_arr_L5 = f_L5 * time_rate_offset\n",
    "\n",
    "misalignment_amount = 0.5\n",
    "time_to_half_chip_L2C = misalignment_amount / code_rate_L2C / time_rate_offset\n",
    "time_to_half_chip_L5 = misalignment_amount / code_rate_L5 / time_rate_offset\n",
    "time_to_half_chip_L2C[num_steps // 2 - 1: num_steps // 2 + 1] = np.nan  # avoid division by zero\n",
    "time_to_half_chip_L5[num_steps // 2 - 1: num_steps // 2 + 1] = np.nan  # avoid division by zero\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(doppler_arr_L2C, time_to_half_chip_L2C * 1e3, label=\"L2C Code\", color=\"blue\")\n",
    "ax.plot(doppler_arr_L5, time_to_half_chip_L5 * 1e3, label=\"L5 Code\", color=\"orange\")\n",
    "ax.set_xlabel(\"Doppler Frequency [Hz]\")\n",
    "ax.set_ylabel(\"Time to ½ Chip Misalignment [ms]\")\n",
    "ax.set_ylim(-2e3, 2e3)\n",
    "ax.set_title(\"Time to ½ Chip Misalignment vs Doppler Frequency\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Lecture 9 Exercise 3\n",
    "\n",
    "What is the computation cost (in terms of number of multiplications and additions) of time domain correlation of 1 ms of GPS L1 CA signal, assuming that the signal is sampled at 5 MHz.\n",
    "\n",
    "(TODO: This exercise needs to be re-written to specify the number of code phase and Doppler bins searched.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Assume that our received signal samples and replica signal are stored in memory.  We ignore memory access time and focus only on the number of arithmetic operations.\n",
    "\n",
    "For $N$ samples, we need $N$ multiplications and $N-1$ additions to compute the correlation value for one code phase.  This result is $\\mathcal{O}(N)$.\n",
    "\n",
    "If we want to compute the correlation for all possible code phases, we need to repeat this for each code phase of interest.  Since 1 ms is equal to one CA code period, and the sampling frequency is 5 MHz, we have $N = 5000$ samples in 1 ms.  If we check each possible code phase at the resolution corresponding to this sampling rate ($\\Delta\\eta = 1.023\\cdot 10^6 / f_\\text{samp} \\approx 0.2$ chips), we have $N$ code phases to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_rate = 5e6\n",
    "chip_rate = 1.023e6\n",
    "code_phase_resolution = chip_rate / samp_rate\n",
    "print(f\"Code phase resolution: {code_phase_resolution} chips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# Lecture 9 Exercise 4 and Lecture 11 Exercise 1\n",
    "Note: Lecture 11 used to be numbered as Lecture 10\n",
    "\n",
    "Implement a function that performs acquisition using the time domain correlation method.  You need to demonstrate that your program works with at least 3 sets of data. For all 3 sets of data, f0 = 1.25e6, and sampling frequency is 5MHZ, bit resolution is 4-bit. Try using the following parameters in your simulated signal: PRN =[4 7 10 15];   CN0 =[45 47 49 45]; n0 = [100 1225 2500 4999];     Initial phase phi = [0 0.5 1.2 1.34] radians;   fd = [1000 1000 2200 3000] Hz;​"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "See `lecture-8/simulate-gps-signal.ipynb` for reference on how to simulate the signals.\n",
    "\n",
    "Here we use a simpler linear signal model to simulate the received signal.\n",
    "\n",
    "The original exercise wording uses code phase in units of samples, denoted as `n0`.  In this notebook, we use code phase in units of seconds or chips, denoted as `eta0`.  Conversion between the two more generally depends on whether Doppler expansion/compression of the code is taken into account.  Here, since we are considering L1 signals with < 5 kHz Doppler, we can ignore this effect for simplicity.\n",
    "\n",
    "```python\n",
    "fs = 5e6  # samples per second\n",
    "chip_rate = 1.023e6  # chips per second\n",
    "n0 = 2500  # code phase in samples\n",
    "eta0 = n0 / fs  # converts code phase from samples to seconds\n",
    "eta0_chips = eta0 * chip_rate  # converts code phase from seconds to chips\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "We implement both the time-domain (Lecture 9 Exercise 4) and frequency-domain (Lecture 10 Exercise 1) correlation methods below.  You can uncomment the frequency-domain correlation code to test it out.  By default, we assume we perform a single 1 ms coherent correlation (M=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import utils.signals.gps_l1ca as gps_l1ca_utils\n",
    "\n",
    "@dataclass\n",
    "class SignalParams:\n",
    "    prn: int\n",
    "    cn0: float  # in dB-Hz\n",
    "    n0: int     # initial sample code phase\n",
    "    initial_phase: float  # in radians\n",
    "    fd: float   # Doppler frequency in Hz\n",
    "\n",
    "signal_params = {\n",
    "    4: SignalParams(prn=4, cn0=45, n0=100, initial_phase=0.0, fd=1000),\n",
    "    7: SignalParams(prn=7, cn0=47, n0=1225, initial_phase=0.5, fd=1000),\n",
    "    10: SignalParams(prn=10, cn0=49, n0=2500, initial_phase=1.2, fd=2200),\n",
    "    15: SignalParams(prn=15, cn0=45, n0=4999, initial_phase=1.34, fd=3000),\n",
    "}\n",
    "\n",
    "samp_rate = 5e6  # 5 MHz\n",
    "sim_duration = 0.1  # seconds\n",
    "sim_time_arr = np.arange(0, sim_duration, 1.0 / samp_rate)\n",
    "sim_num_samples = len(sim_time_arr)\n",
    "\n",
    "# First, compute the noise power based on receiver equivalent noise temperature and front-end bandwidth\n",
    "k_boltzmann = 1.380649e-23  # J/K\n",
    "rx_sys_temp_k = 535.0  # Kelvin\n",
    "front_end_bw_hz = samp_rate  # Hz\n",
    "noise_power_watts = k_boltzmann * rx_sys_temp_k * front_end_bw_hz\n",
    "\n",
    "noise_samples = np.sqrt(noise_power_watts) * (\n",
    "    np.random.randn(sim_num_samples) + 1j * np.random.randn(sim_num_samples)\n",
    ") / np.sqrt(2)\n",
    "\n",
    "noise_free_signal_samples = np.zeros(sim_num_samples, dtype=complex)\n",
    "\n",
    "for prn, params in signal_params.items():\n",
    "    sat_cn0_dBHz = params.cn0\n",
    "    signal_power = noise_power_watts / front_end_bw_hz * 10**(sat_cn0_dBHz / 10)\n",
    "    signal_amp = np.sqrt(signal_power)\n",
    "\n",
    "    code_seq = 1 - 2 * gps_l1ca_utils.generate_code_sequence_L1CA(prn).astype(np.int8)\n",
    "    eta0 = params.n0 / samp_rate  # initial code phase in seconds\n",
    "    code_phase_chips = (eta0 + sim_time_arr) * gps_l1ca_utils.CODE_RATE\n",
    "    code_chip_indices = (code_phase_chips).astype(int) % gps_l1ca_utils.CODE_LENGTH\n",
    "    code_samples = code_seq[code_chip_indices]\n",
    "\n",
    "    num_data_bits = int(np.ceil(sim_duration * gps_l1ca_utils.DATA_SYMBOL_RATE))\n",
    "    data_bits = np.random.choice([-1, 1], size=num_data_bits)\n",
    "    data_bit_indices = ((eta0 + sim_time_arr) * gps_l1ca_utils.DATA_SYMBOL_RATE).astype(int) % num_data_bits\n",
    "    data_samples = data_bits[data_bit_indices]\n",
    "\n",
    "    carrier_phase = 2 * np.pi * (params.fd * sim_time_arr + params.initial_phase / (2 * np.pi))\n",
    "    carrier_samples = np.exp(1j * carrier_phase)\n",
    "\n",
    "    signal_samples = signal_amp * code_samples * data_samples * carrier_samples\n",
    "    noise_free_signal_samples += signal_samples\n",
    "\n",
    "baseband_samples = noise_free_signal_samples + noise_samples\n",
    "\n",
    "num_quant_bits = 4\n",
    "num_quant_levels = 2**num_quant_bits\n",
    "# assume even quantization levels between +/- max_signal_amp\n",
    "#  (sometimes this quantization doesn\"t quite work as expected; it\"s not that important though -- it should be close enough)\n",
    "max_signal_amp = np.max(np.abs(baseband_samples))\n",
    "quantized_samples_real = np.floor(np.real(baseband_samples) / max_signal_amp * (num_quant_levels / 2))\n",
    "quantized_samples_imag = np.floor(np.imag(baseband_samples) / max_signal_amp * (num_quant_levels / 2))\n",
    "quantized_samples = quantized_samples_real + 1j * quantized_samples_imag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delay_doppler_correlate_gps_l1ca_signal(\n",
    "        received_samples: np.ndarray,\n",
    "        prn: int,\n",
    "        min_doppler_hz: float,\n",
    "        max_doppler_hz: float,\n",
    "        samp_rate: float,\n",
    "        doppler_step_hz: float | None = None,\n",
    "        print_progress: bool = True,\n",
    ") -> None:\n",
    "    code_seq = 1 - 2 * gps_l1ca_utils.generate_code_sequence_L1CA(prn).astype(np.int8)\n",
    "    num_samples = len(received_samples)\n",
    "    code_length = gps_l1ca_utils.CODE_LENGTH\n",
    "    code_rate = gps_l1ca_utils.CODE_RATE\n",
    "\n",
    "    if doppler_step_hz is None:\n",
    "        doppler_step_hz = samp_rate / num_samples  # 1 / T resolution\n",
    "    doppler_arr_hz = np.arange(min_doppler_hz, max_doppler_hz + doppler_step_hz, doppler_step_hz)\n",
    "    num_doppler_bins = len(doppler_arr_hz)\n",
    "\n",
    "    time_arr = np.arange(num_samples) / samp_rate\n",
    "    code_seq = 1 - 2 * gps_l1ca_utils.get_GPS_L1CA_code_sequence(prn).astype(np.int8)\n",
    "    code_phase_chips = time_arr * code_rate  # Assume no Doppler expansion/compression\n",
    "    code_chip_indices = (code_phase_chips).astype(int) % code_length\n",
    "    code_samples = code_seq[code_chip_indices]\n",
    "    num_code_phases = len(code_samples)\n",
    "\n",
    "    # Compute FFT of code samples for frequency-domain correlation\n",
    "    # code_samples_fft = np.fft.fft(code_samples)\n",
    "\n",
    "    correlations = np.zeros((num_doppler_bins, num_code_phases), dtype=complex)\n",
    "    for i, doppler_freq in enumerate(doppler_arr_hz):\n",
    "        if print_progress:\n",
    "            print(f\"\\rProcessing Doppler bin {i+1}/{num_doppler_bins}: {doppler_freq} Hz\", end=\"\")\n",
    "        carrier_phase = 2 * np.pi * doppler_freq * time_arr\n",
    "        conj_carrier_samples = np.exp(-1j * carrier_phase)\n",
    "\n",
    "        wipeoff_samples = received_samples * conj_carrier_samples\n",
    "\n",
    "        for j in range(num_code_phases):\n",
    "            shifted_code_samples = np.roll(code_samples, j)\n",
    "            correlations[i, j] = np.sum(wipeoff_samples * shifted_code_samples)\n",
    "\n",
    "        # Frequency-domain correlation\n",
    "        # wipeoff_samples_fft = np.fft.fft(wipeoff_samples)\n",
    "        # corr = np.fft.ifft(wipeoff_samples_fft * np.conj(code_samples_fft))\n",
    "        # correlations[i, :] = corr\n",
    "    if print_progress:\n",
    "        print(\"\\nCorrelation complete.\")\n",
    "\n",
    "    return correlations, doppler_arr_hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "integration_duration = 0.001  # 1 ms\n",
    "num_coh_samples = int(samp_rate * integration_duration)\n",
    "min_doppler_hz = -5000\n",
    "max_doppler_hz = 5000\n",
    "doppler_step_hz = 1 / integration_duration  # 1 kHz\n",
    "correlations, doppler_arr_hz = delay_doppler_correlate_gps_l1ca_signal(\n",
    "    received_samples=quantized_samples[:num_coh_samples],\n",
    "    prn=10,\n",
    "    min_doppler_hz=min_doppler_hz,\n",
    "    max_doppler_hz=max_doppler_hz,\n",
    "    samp_rate=samp_rate,\n",
    "    doppler_step_hz=doppler_step_hz,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "num_doppler_bins, num_code_phases = correlations.shape\n",
    "extent = [0, num_code_phases, min_doppler_hz, max_doppler_hz]\n",
    "\n",
    "norm_corr_power = np.abs(correlations / num_coh_samples)**2\n",
    "noise_power = np.median(norm_corr_power)\n",
    "norm_corr_power = norm_corr_power / noise_power\n",
    "norm_corr_power_dB = 10 * np.log10(norm_corr_power + 1e-12)\n",
    "\n",
    "peak_doppler_idx, peak_code_phase_idx = np.unravel_index(\n",
    "    np.argmax(norm_corr_power_dB),\n",
    "    norm_corr_power_dB.shape,\n",
    ")\n",
    "peak_doppler_hz = doppler_arr_hz[peak_doppler_idx]\n",
    "print(f\"Peak correlation at Doppler: {peak_doppler_hz} Hz, Code Phase: {peak_code_phase_idx} samples\")\n",
    "\n",
    "im = ax.imshow(\n",
    "    norm_corr_power,\n",
    "    extent=extent,\n",
    "    aspect=\"auto\",\n",
    "    interpolation=\"nearest\",\n",
    "    cmap=\"plasma\",\n",
    "    origin=\"lower\",\n",
    "    vmin=noise_power,\n",
    ")\n",
    "code_phase_window = 50  # samples\n",
    "ax.set_xlim(peak_code_phase_idx - code_phase_window, peak_code_phase_idx + code_phase_window)\n",
    "ax.set_xlabel(\"Code Phase [samples]\")\n",
    "ax.set_ylabel(\"Doppler Frequency [Hz]\")\n",
    "ax.set_title(\"Delay-Doppler Correlation Map for PRN 10\")\n",
    "plt.colorbar(im, label=\"Correlation Magnitude [dB]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Lecture 11 Exercise 2\n",
    "\n",
    "With $P_\\text{fa} = 10\\%$ for a single correlation bin, we want to compute the threshold detection level, and we assume a single 1 ms correlation ($M_1$, $T_\\text{coh}=1$ ms).\n",
    "\n",
    "The lecture 10 slides use the Rayleigh distribution to model the coherent correlation noise, and imply that we should use this to compute the threshold.  If we consider $Y=\\sum_{m=1}^M |X_m|^2$ where $X_m$ are independent complex Gaussian random variables with zero mean and variance $\\sigma^2$, then $\\sqrt{Y}$ follows a Chi-squared distribution with $2M$ degrees of freedom.  Thus, the Rayleigh random variable is the square root of a Chi-squared random variable with 2 degrees of freedom (M=1).\n",
    "\n",
    "$$\n",
    "1 - \\text{CDF}(\\text{threshold}) = P_\\text{fa}  \\\\\n",
    "\\Rightarrow \\text{exp}(-y^2 / 2\\sigma^2) = P_\\text{fa} \\\\\n",
    "\\Rightarrow y = \\sigma \\sqrt{-2 \\ln(P_\\text{fa})}\n",
    "$$\n",
    "\n",
    "If $\\sigma_\\epsilon^2 = 1$ is the variance of the noise samples of the correlation outputs $X_i$, then our threshold is simply $y = \\sqrt{-2 \\ln(0.1)} \\approx 2.146$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(-2 * np.log(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Lecture 11 Exercise 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Calculate the probability of detection for a signal with C/N0 values of 20, 25, ..., 50 dB-Hz, assuming 1 ms vs 100 ms coherent integration (M=1) and a false alarm probability of 10%.\n",
    "\n",
    "The distribution of signal plus noise for $Y=\\sum_{m=1}^M |X_m|^2$ is a non-central Chi-squared distribution with 2 degrees of freedom and non-centrality parameter equal to the coherent integration SNR.\n",
    "\n",
    "The slides use $\\sqrt{Y}$, which has a Rician distribution (like a \"non-central Rayleigh\") with parameters $(\\nu, \\sigma)$ where $\\nu = \\sqrt{2 \\cdot \\text{SNR}}$ and $\\sigma^2$ is the variance of the noise samples of the correlation outputs $X_i$.\n",
    "\n",
    "Note: we define our noise power such that $\\sigma^2 = \\frac{1}{2}\\sigma_\\text{Re}^2 = \\frac{1}{2}\\sigma_\\text{Im}^2$ where $\\sigma_\\text{Re}^2$ and $\\sigma_\\text{Im}^2$ are the variances of the real and imaginary parts of the complex Gaussian random variable $X_i$.  If you look on wikipedia, the Rician distribution is defined with $\\sigma^2$ being the variance of the real and imaginary parts, so we need to be careful when comparing formulas.  Also, the pdf formula in the slides I think was wrong last time I looked (should be $1/\\sigma^2$ out front, not $1/\\sqrt{\\sigma^2}$).  Hopefully this will be updated soon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "\n",
    "Our equivalent formula is:\n",
    "\n",
    "$$\n",
    "f(y | \\nu, \\sigma) = \\frac{2y}{\\sigma^2} \\exp(-\\frac{y^2 + A^2}{\\sigma^2}) I_0\\left(\\frac{2y A}{\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "where $I_0$ is the modified Bessel function of the first kind of order zero and $A = \\sqrt{\\text{SNR} \\cdot \\sigma^2}$.\n",
    "\n",
    "\n",
    "Plugging in $\\text{SNR} = A^2 / \\sigma^2$ into our pdf, we get:\n",
    "\n",
    "$$\n",
    "f(y | A, \\sigma) = \\frac{2y}{\\sigma^2} \\exp\\left(-\\frac{y^2}{\\sigma^2}-\\text{SNR}\\right) I_0\\left(\\frac{2y}{\\sigma}\\sqrt{\\text{SNR}}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "The CDF of this distribution does not have a closed form, but it can be computed using the Marcum Q-function:\n",
    "\n",
    "$$\n",
    "\\text{CDF}(y | A, \\sigma) = 1 - Q_1\\left(\\frac{A}{\\sigma}, \\frac{y}{\\sigma}\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "$$\n",
    "Q_1(a, b) = \\int_b^\\infty x \\exp\\left(-\\frac{x^2 + a^2}{2}\\right) I_0(ax) \\, dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "$$\n",
    "P_d = 1 - \\text{CDF}(\\text{threshold} | A, \\sigma) = Q_1\\left(\\frac{A}{\\sigma}, \\frac{\\text{threshold}}{\\sigma}\\right)\n",
    "$$\n",
    "\n",
    "The problem statement implies that we should compute the Marcum Q-function numerically to get the probability of detection.\n",
    "\n",
    "Below, I just use the `scipy.stats` implementation of the rice distribution CDF to compute the probability of detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import rice, rayleigh\n",
    "\n",
    "y = np.arange(0, 10, 0.01)\n",
    "snr = 5.0  # example SNR value\n",
    "noise_var = 1.0\n",
    "sig_ampl = np.sqrt(snr * noise_var)\n",
    "rayl_pdf_vals = rayleigh.pdf(y, scale=np.sqrt(noise_var))\n",
    "rice_pdf_vals = rice.pdf(y, b=np.sqrt(snr), scale=np.sqrt(noise_var))\n",
    "\n",
    "prob_fa = 0.1\n",
    "threshold = np.sqrt(-2 * noise_var * np.log(prob_fa))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(y, rayl_pdf_vals, label=f\"noise (Rayleigh)\", color=\"blue\")\n",
    "ax.plot(y, rice_pdf_vals, label=f\"signal+noise (Rice)\", color=\"orange\")\n",
    "ax.vlines(sig_ampl, 0, np.max(rice_pdf_vals), colors=\"red\", label=\"A\")\n",
    "ax.vlines(threshold, 0, np.max(rayl_pdf_vals), colors=\"k\", linestyles=\"dashed\", lw=3, label=\"threshold\")\n",
    "ax.set_xlabel(\"Amplitude\")\n",
    "ax.set_ylabel(\"Probability Density Function (PDF)\")\n",
    "ax.set_title(\"Rice Distribution PDF\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "cn0_arr = np.arange(20, 51, 5)  # 20 to 50 dB-Hz\n",
    "false_alarm_prob = 0.1\n",
    "\n",
    "noise_var = 1.0  # assume noise variance of 1 for correlation outputs\n",
    "for T_coh in [1e-3, 10e-3, 100e-3]:  # 1 ms and 100 ms\n",
    "    for i, cn0_dBHz in enumerate(cn0_arr):\n",
    "        cn0_linear = 10**(cn0_dBHz / 10)\n",
    "        SNR_coh = cn0_linear * T_coh\n",
    "        A = np.sqrt(SNR_coh * noise_var)\n",
    "\n",
    "        threshold = np.sqrt(-2 * noise_var * np.log(false_alarm_prob))\n",
    "\n",
    "        # Probability of detection using Marcum Q-function\n",
    "        prob_detection = 1 - rice.cdf(threshold, b=A, scale=np.sqrt(noise_var))\n",
    "        print(f\"T_coh: {T_coh*1e3:.1f} ms, C/N0: {cn0_dBHz} dB-Hz, P_D: {prob_detection:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnss_lectures",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
